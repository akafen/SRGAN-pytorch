import os
import torch
import torchvision
import argparse
from vgg import *
from setproctitle import *

parser.argsparse.ArgumentParser(description='this script is wrote for fine tunning vgg before trainning SRGAN')
parser.add_argument('-j', '--workers', default=4, type=int,
        help='number of data loading workers (default: 4)')
parser.add_argument('--fixF',action='store_true',
        help='weather to fix feature layers of vgg')
parser.add_argument('--batch-size','-b',default=8)
parser.add_argument('--epochs', default=40, type=int,
        help='number of total epochs to run')
parser.add_argument('--crop-size','-c',default=256,type=int,
        help='crop size of the image')
parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,
        help='initial learning rate')
parser.add_argument('--print-freq', '-p', default=100, type=int,
        help='print frequency (default: 20)')
parser.add_argument('--resume', default='', type=str,
        help='path to latest checkpoint (default: none)')
parser.add_argument('--logdir','-s',default='save',type=str,
        help='path to save checkpoint')
parser.add_argument('--traindir',default='r375-400.bin',
        help=' the global name of training set dir')
parser.add_argument('--gen', default=None,type=str,
        help='path to generator for producing fake image')

args = parser.parse_args()
args.__dict__['upscale_factor']=4
#args.traindir=globals()[args.traindir]
args.__dict__['model_base_name']='vgg_v%g'%(args.lr)+('_fixF' if args.fixF else '')
args.__dict__['model_name']=args.model_base_name+'.pth'

setproctitle(args.model_base_name)
if not os.path.exists(args.logdir):
    os.makedirs(args.logdir)

cudnn.benchmark = True

train_loader = torch.utils.data.DataLoader(
    get_train_set(args),
    batch_size=args.batch_size, shuffle=True,
    num_workers=args.workers, pin_memory=True)

gen=GenNet().cuda()
vgg=vgg19_54().cuda()

if args.resume:
    if os.path.isfile(args.resume):
        print("=> loading checkpoint '{}'".format(args.resume))
        checkpoint = torch.load(args.resume)
        global_step=checkpoint['global_step']
        gen.load_state_dict(checkpoint['gen_state_dict'])
        vgg.load_state_dict(checkpoint['vgg_state_dict'])
        print("=> loaded checkpoint ")
    else:
        print("=> no checkpoint found at '{}'".format(args.resume))

if args.generator:
    if os.path.isfile(args.generator):
        print("=> loading checkpoint '{}'".format(args.generator))
        checkpoint = torch.load(args.generator)
        gen.load_state_dict(checkpoint['state_dict'])
        print("=> loaded checkpoint")
    else:
        print("=>no checkpoint found at '{}'".format(args.generator))

if args.fixF:
    vgg_optimizer = torch.optim.Adam(vgg.classifier.parameters(), args.lr)
else:
    vgg_optimizer = torch.optim.Adam(vgg.parameters(),args.lr)

def normalize(tensor):
    r,g,b=torch.split(tensor,1,1)
    r=(r-0.485)/0.229
    g=(g-0.456)/0.224
    b=(b-0.406)/0.225
    return torch.cat((r,g,b),1)

def save_checkpoint(state, is_best,logdir):
    filename=os.path.join(logdir,args.model_name)
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, os.path.join(logdir,'best_'+args.model_name))

def train(epoch):
    for i, (input, target) in enumerate(train_loader):
        global global_step
        if(global_step%1000==0):
            for pg in gen_optimizer.param_groups:
                pg['lr']=pg['lr']*0.915
            for pg in disc_optimizer.param_groups:
                pg['lr']=pg['lr']*0.915
        
        input_var = Variable(input.cuda())
        target_var = Variable(target.cuda())
        
        vgg_optimizer.zero_grad()
        output = gen(input_var).detach()

            disc_optimizer.zero_grad()
            real_output=disc(target_var)
            real_loss=((real_output-1)**2).mean()
            real_loss.backward()
            fake_output=disc(gen(input_var).detach())
            fake_loss=(fake_output**2).mean()
            fake_loss.backward()
            disc_loss=fake_loss+real_loss
            disc_optimizer.step()
            
        if not args.fixG:
            gen_optimizer.zero_grad()
            G_z=gen(input_var)
            x1,x2,x3,x4,x5,x6=vgg(normalize(G_z))
            y1,y2,y3,y4,y5,y6=vgg(normalize(target_var))
            output=disc(G_z)
            adv_loss=((output-1)**2).mean()
            content_loss=cont_criterion(x1,y1.detach())\
                     +cont_criterion(x2,y2.detach())\
                     +cont_criterion(x3,y3.detach())\
                     +cont_criterion(x4,y4.detach())\
                     +cont_criterion(x5,y5.detach())\
                     +cont_criterion(x6,y6.detach())\
                     +adv_loss
            #content_loss=cont_criterion(fake_feature,real_feature)
            gen_loss=content_loss
            gen_loss.backward()
            gen_optimizer.step()
            
        if i % args.print_freq == 0:
            s=time.strftime('%dth-%H:%M:%S',time.localtime(time.time()))+' | epoch%d(%d) | lr=%g'%(epoch,global_step,args.lr)
            if not args.fixG:
                vs=validate(gen,vgg,cont_criterion,args.valdir,epoch,args.upscale_factor,gen_optimizer)
                s+=vs+' | Loss(G):%g[Cont:%g/Adv:%g]'%(gen_loss.data[0],content_loss.data[0],adv_loss.data[0])
            if not args.fixD:
                s+=' | Loss(D):%g[Real:%g/Fake:%g]'%(disc_loss.data[0],real_loss.data[0],fake_loss.data[0])
            print(s)
            f=open('info.'+args.model_base_name,'a')
            f.write(s+'\n')
            f.close()

            if not args.fixG:
                global best_gen_loss
                is_best = gen_loss.data[0] < best_gen_loss
                best_gen_loss = min(gen_loss.data[0] , best_gen_loss)
            else:
                global best_disc_loss
                is_best = disc_loss.data[0] < best_disc_loss
                best_disc_loss = min(disc_loss.data[0],best_disc_loss)
                
            save_checkpoint({
                'epoch': epoch + 1,
                'global_step':global_step,
                'gen_state_dict': gen.state_dict(),
                'disc_state_dict':disc.state_dict(),
            }, is_best,args.logdir)

        global_step+=1

for epoch in range(args.start_epoch, args.epochs):
    train(epoch)
